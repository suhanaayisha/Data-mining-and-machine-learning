{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6a76e0-13b8-4de2-b59d-8a8a0d5412e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_curve)\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08989501-4640-4858-ad5b-e82bf804c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8bb9a0d-6866-4c7b-af58-03e5b202248a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"../x_train_all.csv\")\n",
    "X_test = pd.read_csv(\"../x_test_all.csv\")\n",
    "y_train = pd.read_csv(\"../y_train_all.csv\")\n",
    "y_test = pd.read_csv(\"../y_test_all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be0ae8d-af54-4195-ad20-ee8a414a5dc0",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "266532df-7dde-4a6a-94ef-5455141f7ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, f_classif\n",
    "def selectFTest(top_features, range_arg, x_train_arg, y_train_arg, x_test_arg, y_test_arg):\n",
    "    selected_features = []\n",
    "\n",
    "    for y_class in range(range_arg):\n",
    "        selector = SelectKBest(score_func=f_classif, k=top_features)\n",
    "        selector.fit(x_train_arg, (y_train_arg == y_class).astype(int).values.ravel())\n",
    "        selected_indices = selector.get_support(indices=True)\n",
    "        \n",
    "        # selected_features.update(selected_indices)\n",
    "        selected_features.extend(selected_indices)\n",
    "\n",
    "    return x_train_arg.iloc[:, selected_features], x_test_arg.iloc[:, selected_features]\n",
    "\n",
    "X_train_50, X_test_50 = selectFTest(5, 10, X_train, y_train, X_test, y_test) # 50 features\n",
    "X_train_100, X_test_100 = selectFTest(10, 10, X_train, y_train, X_test, y_test) # 100 features\n",
    "X_train_200, X_test_200 = selectFTest(20, 10, X_train, y_train, X_test, y_test) # 200 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "853dea82-b268-4a2d-b799-817e663eb411",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2213</th>\n",
       "      <th>2260</th>\n",
       "      <th>2261</th>\n",
       "      <th>2262</th>\n",
       "      <th>2263</th>\n",
       "      <th>1072</th>\n",
       "      <th>1073</th>\n",
       "      <th>1074</th>\n",
       "      <th>1120</th>\n",
       "      <th>1121</th>\n",
       "      <th>...</th>\n",
       "      <th>1666</th>\n",
       "      <th>1714</th>\n",
       "      <th>1715</th>\n",
       "      <th>1743</th>\n",
       "      <th>1761</th>\n",
       "      <th>1086</th>\n",
       "      <th>1134</th>\n",
       "      <th>1215</th>\n",
       "      <th>1216</th>\n",
       "      <th>1263</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>...</td>\n",
       "      <td>102.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>...</td>\n",
       "      <td>101.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>208.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>...</td>\n",
       "      <td>113.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>191.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>138.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>...</td>\n",
       "      <td>104.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>209.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9685</th>\n",
       "      <td>42.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9686</th>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9687</th>\n",
       "      <td>30.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9688</th>\n",
       "      <td>36.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>93.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9689</th>\n",
       "      <td>34.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>88.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9690 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       2213   2260   2261   2262   2263   1072   1073   1074   1120   1121  \\\n",
       "0      73.0   73.0   72.0   71.0   68.0  184.0  174.0  163.0  207.0  197.0   \n",
       "1      89.0   91.0   85.0   76.0   70.0  169.0  192.0  195.0  183.0  203.0   \n",
       "2      92.0   90.0   81.0   71.0   66.0  196.0  191.0  179.0  211.0  207.0   \n",
       "3     100.0   86.0   92.0   81.0   72.0  213.0  204.0  207.0  227.0  218.0   \n",
       "4     138.0  123.0  126.0  127.0  128.0  163.0  162.0  150.0  178.0  170.0   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "9685   42.0   40.0   41.0   42.0   42.0   18.0   18.0   47.0   18.0   20.0   \n",
       "9686   36.0   36.0   36.0   35.0   32.0   17.0   18.0   78.0   18.0   33.0   \n",
       "9687   30.0   33.0   29.0   31.0   30.0   17.0   19.0   83.0   17.0   45.0   \n",
       "9688   36.0   39.0   36.0   37.0   32.0   16.0   16.0   77.0   16.0   31.0   \n",
       "9689   34.0   37.0   32.0   31.0   33.0   15.0   16.0   76.0   15.0   31.0   \n",
       "\n",
       "      ...   1666   1714   1715   1743   1761   1086   1134   1215   1216  \\\n",
       "0     ...  102.0   99.0   95.0  118.0  108.0  123.0  127.0  220.0  231.0   \n",
       "1     ...   99.0  118.0  111.0   98.0  133.0  185.0  193.0  124.0  199.0   \n",
       "2     ...  101.0  122.0  114.0  112.0  136.0  161.0  167.0  196.0  225.0   \n",
       "3     ...  113.0  124.0  116.0  105.0  134.0  177.0  179.0  206.0  230.0   \n",
       "4     ...  104.0   87.0   81.0   92.0  102.0  139.0  186.0  199.0  202.0   \n",
       "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "9685  ...   99.0  104.0   80.0   94.0  106.0   18.0   21.0   19.0   22.0   \n",
       "9686  ...   98.0  101.0   82.0   82.0  104.0   18.0   24.0   19.0   30.0   \n",
       "9687  ...   98.0   99.0   95.0   84.0  102.0   19.0   25.0   18.0   36.0   \n",
       "9688  ...   93.0   92.0   88.0   93.0   93.0   19.0   24.0   16.0   25.0   \n",
       "9689  ...   88.0   88.0   84.0   85.0   89.0   15.0   19.0   15.0   26.0   \n",
       "\n",
       "       1263  \n",
       "0     226.0  \n",
       "1     123.0  \n",
       "2     208.0  \n",
       "3     191.0  \n",
       "4     209.0  \n",
       "...     ...  \n",
       "9685   19.0  \n",
       "9686   20.0  \n",
       "9687   18.0  \n",
       "9688   17.0  \n",
       "9689   16.0  \n",
       "\n",
       "[9690 rows x 50 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "058c0d03-dcf5-4b2e-b02e-4ba34d6b8294",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2165</th>\n",
       "      <th>2212</th>\n",
       "      <th>2213</th>\n",
       "      <th>2214</th>\n",
       "      <th>2215</th>\n",
       "      <th>2260</th>\n",
       "      <th>2261</th>\n",
       "      <th>2262</th>\n",
       "      <th>2263</th>\n",
       "      <th>2264</th>\n",
       "      <th>...</th>\n",
       "      <th>1086</th>\n",
       "      <th>1134</th>\n",
       "      <th>1167</th>\n",
       "      <th>1168</th>\n",
       "      <th>1215</th>\n",
       "      <th>1216</th>\n",
       "      <th>1263</th>\n",
       "      <th>1561</th>\n",
       "      <th>1562</th>\n",
       "      <th>1610</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>123.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>236.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>...</td>\n",
       "      <td>185.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>237.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>...</td>\n",
       "      <td>161.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>233.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>...</td>\n",
       "      <td>177.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>144.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>...</td>\n",
       "      <td>139.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>214.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9685</th>\n",
       "      <td>41.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9686</th>\n",
       "      <td>34.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9687</th>\n",
       "      <td>32.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9688</th>\n",
       "      <td>45.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9689</th>\n",
       "      <td>33.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9690 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       2165   2212   2213   2214   2215   2260   2261   2262   2263   2264  \\\n",
       "0      77.0   77.0   73.0   72.0   72.0   73.0   72.0   71.0   68.0   75.0   \n",
       "1      98.0   94.0   89.0   85.0   81.0   91.0   85.0   76.0   70.0   71.0   \n",
       "2     108.0   99.0   92.0   86.0   81.0   90.0   81.0   71.0   66.0   68.0   \n",
       "3     111.0   96.0  100.0   87.0   80.0   86.0   92.0   81.0   72.0   63.0   \n",
       "4     144.0  136.0  138.0  131.0  121.0  123.0  126.0  127.0  128.0  129.0   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "9685   41.0   40.0   42.0   43.0   45.0   40.0   41.0   42.0   42.0   40.0   \n",
       "9686   34.0   38.0   36.0   36.0   33.0   36.0   36.0   35.0   32.0   30.0   \n",
       "9687   32.0   35.0   30.0   32.0   29.0   33.0   29.0   31.0   30.0   27.0   \n",
       "9688   45.0   46.0   36.0   40.0   34.0   39.0   36.0   37.0   32.0   32.0   \n",
       "9689   33.0   37.0   34.0   32.0   32.0   37.0   32.0   31.0   33.0   33.0   \n",
       "\n",
       "      ...   1086   1134   1167   1168   1215   1216   1263   1561   1562  \\\n",
       "0     ...  123.0  127.0  211.0  224.0  220.0  231.0  226.0  225.0  225.0   \n",
       "1     ...  185.0  193.0  122.0  198.0  124.0  199.0  123.0  231.0  232.0   \n",
       "2     ...  161.0  167.0  189.0  220.0  196.0  225.0  208.0  234.0  235.0   \n",
       "3     ...  177.0  179.0  202.0  233.0  206.0  230.0  191.0  237.0  237.0   \n",
       "4     ...  139.0  186.0  186.0  180.0  199.0  202.0  209.0  152.0  169.0   \n",
       "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "9685  ...   18.0   21.0   20.0   19.0   19.0   22.0   19.0   50.0   44.0   \n",
       "9686  ...   18.0   24.0   18.0   19.0   19.0   30.0   20.0   41.0   40.0   \n",
       "9687  ...   19.0   25.0   17.0   18.0   18.0   36.0   18.0   48.0   40.0   \n",
       "9688  ...   19.0   24.0   16.0   16.0   16.0   25.0   17.0   36.0   31.0   \n",
       "9689  ...   15.0   19.0   14.0   15.0   15.0   26.0   16.0   36.0   29.0   \n",
       "\n",
       "       1610  \n",
       "0     236.0  \n",
       "1     237.0  \n",
       "2     233.0  \n",
       "3     217.0  \n",
       "4     214.0  \n",
       "...     ...  \n",
       "9685   36.0  \n",
       "9686   37.0  \n",
       "9687   33.0  \n",
       "9688   23.0  \n",
       "9689   29.0  \n",
       "\n",
       "[9690 rows x 100 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c24e2e5-430f-45a8-b9ee-3ef1cb8b9d2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1979</th>\n",
       "      <th>2025</th>\n",
       "      <th>2026</th>\n",
       "      <th>2027</th>\n",
       "      <th>2028</th>\n",
       "      <th>2164</th>\n",
       "      <th>2165</th>\n",
       "      <th>2166</th>\n",
       "      <th>2211</th>\n",
       "      <th>2212</th>\n",
       "      <th>...</th>\n",
       "      <th>1214</th>\n",
       "      <th>1215</th>\n",
       "      <th>1216</th>\n",
       "      <th>1262</th>\n",
       "      <th>1263</th>\n",
       "      <th>1264</th>\n",
       "      <th>1311</th>\n",
       "      <th>1561</th>\n",
       "      <th>1562</th>\n",
       "      <th>1610</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>168.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>236.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>139.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>95.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>237.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>...</td>\n",
       "      <td>147.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>233.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>146.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>...</td>\n",
       "      <td>167.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>...</td>\n",
       "      <td>162.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>214.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9685</th>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9686</th>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9687</th>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9688</th>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9689</th>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9690 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1979   2025   2026   2027   2028   2164   2165   2166   2211   2212  \\\n",
       "0     130.0  155.0  150.0  146.0  143.0   85.0   77.0   76.0   87.0   77.0   \n",
       "1     139.0  117.0  139.0  144.0  144.0  100.0   98.0   99.0   95.0   94.0   \n",
       "2     132.0  127.0  147.0  138.0  131.0  110.0  108.0  105.0  100.0   99.0   \n",
       "3     146.0  121.0  140.0  144.0  143.0  108.0  111.0   97.0   92.0   96.0   \n",
       "4     141.0   89.0  125.0  142.0  137.0  142.0  144.0  132.0  136.0  136.0   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "9685   18.0   19.0   20.0   20.0   21.0   42.0   41.0   44.0   38.0   40.0   \n",
       "9686   18.0   18.0   18.0   18.0   18.0   39.0   34.0   36.0   35.0   38.0   \n",
       "9687   17.0   18.0   18.0   18.0   19.0   37.0   32.0   30.0   32.0   35.0   \n",
       "9688   16.0   17.0   17.0   17.0   17.0   39.0   45.0   38.0   49.0   46.0   \n",
       "9689   15.0   18.0   18.0   19.0   19.0   36.0   33.0   33.0   33.0   37.0   \n",
       "\n",
       "      ...   1214   1215   1216   1262   1263   1264   1311   1561   1562  \\\n",
       "0     ...  168.0  220.0  231.0  176.0  226.0  234.0  213.0  225.0  225.0   \n",
       "1     ...   95.0  124.0  199.0  100.0  123.0  194.0  113.0  231.0  232.0   \n",
       "2     ...  147.0  196.0  225.0  153.0  208.0  227.0  180.0  234.0  235.0   \n",
       "3     ...  167.0  206.0  230.0  142.0  191.0  225.0  170.0  237.0  237.0   \n",
       "4     ...  162.0  199.0  202.0  158.0  209.0  224.0  216.0  152.0  169.0   \n",
       "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "9685  ...   21.0   19.0   22.0   20.0   19.0   38.0   22.0   50.0   44.0   \n",
       "9686  ...   19.0   19.0   30.0   19.0   20.0   60.0   26.0   41.0   40.0   \n",
       "9687  ...   17.0   18.0   36.0   17.0   18.0   80.0   36.0   48.0   40.0   \n",
       "9688  ...   16.0   16.0   25.0   16.0   17.0   58.0   21.0   36.0   31.0   \n",
       "9689  ...   14.0   15.0   26.0   15.0   16.0   66.0   23.0   36.0   29.0   \n",
       "\n",
       "       1610  \n",
       "0     236.0  \n",
       "1     237.0  \n",
       "2     233.0  \n",
       "3     217.0  \n",
       "4     214.0  \n",
       "...     ...  \n",
       "9685   36.0  \n",
       "9686   37.0  \n",
       "9687   33.0  \n",
       "9688   23.0  \n",
       "9689   29.0  \n",
       "\n",
       "[9690 rows x 200 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8190722-1b5f-49d0-b88b-ea3ae7b56a89",
   "metadata": {},
   "source": [
    "# Evaluating using a linear classifier, i.e. perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f9357ac-7865-46f1-84b7-944427a89628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perceptron = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f9fd431-430b-4abf-a035-8f69b6877268",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy with cross-validation for a perceptron with 50 selected features is: 0.6954\n",
      "Mean accuracy with cross-validation for a perceptron with 100 selected features is: 0.7535\n",
      "Mean accuracy with cross-validation for a perceptron with 200 selected features is: 0.7674\n"
     ]
    }
   ],
   "source": [
    "def print_perceptron_cv_accuracy(X_train, X_test, y_train, y_test):\n",
    "    cv_scores = cross_val_score(perceptron, X_train, y_train.values.ravel(), cv=10)  \n",
    "    cv_mean_accuracy = np.mean(cv_scores)\n",
    "    print(f'Mean accuracy with cross-validation for a perceptron with {len(X_train.columns)} selected features is: {cv_mean_accuracy:.4f}')\n",
    "    \n",
    "print_perceptron_cv_accuracy(X_train_50, X_test_50, y_train, y_test)\n",
    "print_perceptron_cv_accuracy(X_train_100, X_test_100, y_train, y_test)\n",
    "print_perceptron_cv_accuracy(X_train_200, X_test_200, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8bcf61c-337f-4243-afe4-7dcf1c6b3e59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without cross-validation for a perceptron with 50 selected features is: 0.6647\n",
      "Accuracy without cross-validation for a perceptron with 100 selected features is: 0.7848\n",
      "Accuracy without cross-validation for a perceptron with 200 selected features is: 0.7828\n"
     ]
    }
   ],
   "source": [
    "def print_perceptron_noncv_accuracy(X_train, X_test, y_train, y_test):\n",
    "    perceptron.fit(X_train, y_train.values.ravel())\n",
    "    y_pred = perceptron.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f'Accuracy without cross-validation for a perceptron with {len(X_train.columns)} selected features is: {accuracy:.4f}')\n",
    "    \n",
    "print_perceptron_noncv_accuracy(X_train_50, X_test_50, y_train, y_test)\n",
    "print_perceptron_noncv_accuracy(X_train_100, X_test_100, y_train, y_test)\n",
    "print_perceptron_noncv_accuracy(X_train_200, X_test_200, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3538055-1330-42e4-b791-5d3cc77dc239",
   "metadata": {},
   "source": [
    "At first glance, we can see that the performance of the perceptron for both the 10 fold cross validation as well as with the training and testing datasets are quite similar. However, the 60-70% accuracy provides little conclusive information on whether the data is truly linearly separable. We can still guess that the data is mostly likely not linearly separable as there seems to be some additional noise regarding the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a1f3bd-9dc3-408f-b2b4-fc1efca841ef",
   "metadata": {},
   "source": [
    "# Evaluating using a multilayer perceptron from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0f5362-ca2e-45cf-bd9d-8dabb45d343c",
   "metadata": {},
   "source": [
    "We use the MLPClassifier model provided by sklearn as it seems to suit most of our needs. To establish a baseline for comparison, we first run on a default model that uses the \"adam\" solver as well using 5 hidden layers as well as allowing a maaximum of 100 iterations for training. For every test we do from this point, we pass in the three groups of datasets to the models that each contain 50, 100 and 200 selected features respectively to allow for a larger frame of reference for comparing model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8efd83a-6121-42de-9495-b097187754db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_mlp_accuracy(model, X_train, X_test, y_train, y_test, iterations = 3):\n",
    "    accuracy = 0\n",
    "\n",
    "    for x in range(iterations):\n",
    "        model.fit(X_train, y_train.values.ravel())\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy += accuracy_score(y_test, y_pred)\n",
    "\n",
    "    accuracy /= iterations\n",
    "    print(f'Average accuracy using an MLP over {iterations} runs with {len(X_train.columns)} selected features is: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b75dacc-5390-4472-b5f2-0d69d75412f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 50 selected features is: 0.4124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 100 selected features is: 0.2472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 200 selected features is: 0.2447\n"
     ]
    }
   ],
   "source": [
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(5,), max_iter=100)\n",
    "\n",
    "print_mlp_accuracy(mlp_classifier, X_train_50, X_test_50, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_100, X_test_100, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_200, X_test_200, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915953d9-e164-43b1-9461-829f8f95b716",
   "metadata": {},
   "source": [
    "On noticing the \"hasn't converged yet\" warning, we now try with a larger amount of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "897d326d-d49f-4c26-9ab4-83e3ffff5235",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 50 selected features is: 0.5152\n",
      "Average accuracy using an MLP over 3 runs with 100 selected features is: 0.2947\n",
      "Average accuracy using an MLP over 3 runs with 200 selected features is: 0.2434\n"
     ]
    }
   ],
   "source": [
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(5,), max_iter=500)\n",
    "\n",
    "print_mlp_accuracy(mlp_classifier, X_train_50, X_test_50, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_100, X_test_100, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_200, X_test_200, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c789885-0f43-49e4-acbd-5a345d677529",
   "metadata": {},
   "source": [
    "On keeping the hidden layer size the same and increasing the iterations allowed, we see that the 50 selected features dataset has a higher accuracy while noticing a similar value for the 200 selected features dataset and a dip in accuracy for the 100 selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3630b0b-a1ed-4876-a77a-691a0e6ffb8a",
   "metadata": {},
   "source": [
    "# Checking the effects of increasing the nuumber of iterations allowed on an MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38853aa1-9a77-44c9-9f41-1d8bf69a2dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 50 selected features is: 0.3953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 100 selected features is: 0.3523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 200 selected features is: 0.3275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(10,), max_iter=50)\n",
    "\n",
    "print_mlp_accuracy(mlp_classifier, X_train_50, X_test_50, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_100, X_test_100, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_200, X_test_200, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bab487f-4294-4aab-aae9-c7b30c0366b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 50 selected features is: 0.6475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 100 selected features is: 0.3904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 200 selected features is: 0.3454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(10,), max_iter=100) \n",
    "\n",
    "print_mlp_accuracy(mlp_classifier, X_train_50, X_test_50, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_100, X_test_100, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_200, X_test_200, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c116ba44-dd1d-43f7-a000-7c95ab67563a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 50 selected features is: 0.7648\n",
      "Average accuracy using an MLP over 3 runs with 100 selected features is: 0.7886\n",
      "Average accuracy using an MLP over 3 runs with 200 selected features is: 0.5858\n"
     ]
    }
   ],
   "source": [
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(10,), max_iter=500) \n",
    "\n",
    "print_mlp_accuracy(mlp_classifier, X_train_50, X_test_50, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_100, X_test_100, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_200, X_test_200, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b37c62a-0e5a-4163-b440-681ca1d70308",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 50 selected features is: 0.7840\n",
      "Average accuracy using an MLP over 3 runs with 100 selected features is: 0.6248\n",
      "Average accuracy using an MLP over 3 runs with 200 selected features is: 0.5360\n"
     ]
    }
   ],
   "source": [
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000) \n",
    "\n",
    "print_mlp_accuracy(mlp_classifier, X_train_50, X_test_50, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_100, X_test_100, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_200, X_test_200, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfae975f-d4d3-4b27-bc0c-accd5527ccf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 50 selected features is: 0.7781\n",
      "Average accuracy using an MLP over 3 runs with 100 selected features is: 0.8147\n",
      "Average accuracy using an MLP over 3 runs with 200 selected features is: 0.6661\n"
     ]
    }
   ],
   "source": [
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(10,), max_iter=2000) \n",
    "\n",
    "print_mlp_accuracy(mlp_classifier, X_train_50, X_test_50, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_100, X_test_100, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_200, X_test_200, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6ba00-73d4-4270-a14d-68ea627c2ac4",
   "metadata": {},
   "source": [
    "For iteration values 50 and 100, we see that the MLP does not converge and also notice low accuracy results except for the 100 selected features with 100 iterations. We also see that on reaching ~500 max_iterations, there is lesser returns on the accuracy on average. However, we cannot conclude that this means that 500 maxmimum iterations is the optimal number for this dataset as it the optimization may go differently with other values for the hidden layer sizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb2694-1558-4dfc-9c5c-038a98001169",
   "metadata": {},
   "source": [
    "# Checking the effects of increasing the nuumber of hidden layers of an MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2376d742-23c9-4d43-a370-c57cae6ba4f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 50 selected features is: 0.8204\n",
      "Average accuracy using an MLP over 3 runs with 100 selected features is: 0.8447\n",
      "Average accuracy using an MLP over 3 runs with 200 selected features is: 0.6697\n"
     ]
    }
   ],
   "source": [
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(20,), max_iter=500)\n",
    "\n",
    "print_mlp_accuracy(mlp_classifier, X_train_50, X_test_50, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_100, X_test_100, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_200, X_test_200, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8230194c-cb01-4caa-8c03-9d8935f565fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 50 selected features is: 0.8440\n",
      "Average accuracy using an MLP over 3 runs with 100 selected features is: 0.8961\n",
      "Average accuracy using an MLP over 3 runs with 200 selected features is: 0.8873\n"
     ]
    }
   ],
   "source": [
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(50,), max_iter=500)\n",
    "\n",
    "print_mlp_accuracy(mlp_classifier, X_train_50, X_test_50, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_100, X_test_100, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_200, X_test_200, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3a79a26-4f5f-4bcc-bb64-adaa9138e8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 50 selected features is: 0.8467\n",
      "Average accuracy using an MLP over 3 runs with 100 selected features is: 0.8764\n",
      "Average accuracy using an MLP over 3 runs with 200 selected features is: 0.8931\n"
     ]
    }
   ],
   "source": [
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(70,), max_iter=500)\n",
    "\n",
    "print_mlp_accuracy(mlp_classifier, X_train_50, X_test_50, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_100, X_test_100, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_200, X_test_200, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec57eb-db00-449a-8035-01d250f8cdcd",
   "metadata": {},
   "source": [
    "We notice that there is an increase in performance upto an amount of hidden layer sizes (50) after which, the return in performance seems to be minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cec7cd6-16ab-4552-824a-3a70da268a0f",
   "metadata": {},
   "source": [
    "# Testing different activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f99aa95-e83b-4921-a3e2-959ca3c340d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 50 selected features is: 0.8247\n",
      "Average accuracy using an MLP over 3 runs with 100 selected features is: 0.8434\n",
      "Average accuracy using an MLP over 3 runs with 200 selected features is: 0.8300\n"
     ]
    }
   ],
   "source": [
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(15,), max_iter=500, activation='logistic')\n",
    "\n",
    "print_mlp_accuracy(mlp_classifier, X_train_50, X_test_50, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_100, X_test_100, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_200, X_test_200, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "803e2524-cf26-4b54-95c3-0c33ca40b3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 50 selected features is: 0.8436\n",
      "Average accuracy using an MLP over 3 runs with 100 selected features is: 0.8585\n",
      "Average accuracy using an MLP over 3 runs with 200 selected features is: 0.8383\n"
     ]
    }
   ],
   "source": [
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(30,), max_iter=500, activation='logistic')\n",
    "\n",
    "print_mlp_accuracy(mlp_classifier, X_train_50, X_test_50, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_100, X_test_100, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_200, X_test_200, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "733fa530-4840-4e7f-9ac0-59cbc96ced0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 50 selected features is: 0.7466\n",
      "Average accuracy using an MLP over 3 runs with 100 selected features is: 0.7817\n",
      "Average accuracy using an MLP over 3 runs with 200 selected features is: 0.7254\n"
     ]
    }
   ],
   "source": [
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(15,), max_iter=500, activation='tanh')\n",
    "\n",
    "print_mlp_accuracy(mlp_classifier, X_train_50, X_test_50, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_100, X_test_100, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_200, X_test_200, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb5ba016-9004-4cae-926c-ae70d620d984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy using an MLP over 3 runs with 50 selected features is: 0.7970\n",
      "Average accuracy using an MLP over 3 runs with 100 selected features is: 0.8145\n",
      "Average accuracy using an MLP over 3 runs with 200 selected features is: 0.8138\n"
     ]
    }
   ],
   "source": [
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(30,), max_iter=500, activation='tanh')\n",
    "\n",
    "print_mlp_accuracy(mlp_classifier, X_train_50, X_test_50, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_100, X_test_100, y_train, y_test)\n",
    "print_mlp_accuracy(mlp_classifier, X_train_200, X_test_200, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0028752c-39de-46d1-83eb-6526e5df32de",
   "metadata": {},
   "source": [
    "# Testing using a keras MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291bf69c-61f8-4c94-bae2-71773cbae018",
   "metadata": {},
   "source": [
    "Now that we are testing with the keras model, we have more finer control over the number of neurons per layer as well as the activation functions used. Again, as a baseline we use 50 neurons per hidden layer while running it over 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eaed8e6e-282c-4371-9f94-33888a2d4d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test, epoch):\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=epoch)\n",
    "    \n",
    "    history = model.predict(X_test)\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    print(f'Accuracy of MLP trained over {epoch} epochs is: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1abefe26-ce64-4467-9d04-a4c7771e4087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 8.1294 - accuracy: 0.2262\n",
      "Epoch 2/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.0943 - accuracy: 0.2275\n",
      "Epoch 3/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9839 - accuracy: 0.2333\n",
      "Epoch 4/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9332 - accuracy: 0.2298\n",
      "Epoch 5/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9169 - accuracy: 0.2344\n",
      "Epoch 6/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9127 - accuracy: 0.2357\n",
      "Epoch 7/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9108 - accuracy: 0.2377\n",
      "Epoch 8/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9102 - accuracy: 0.2354\n",
      "Epoch 9/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9084 - accuracy: 0.2348\n",
      "Epoch 10/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.9096 - accuracy: 0.2391\n",
      "97/97 [==============================] - 0s 1ms/step\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 1.8872 - accuracy: 0.2421\n",
      "Accuracy of MLP trained over 10 epochs is: 0.2421\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(50,)),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "train_and_evaluate(model, X_train_50, X_test_50, y_train, y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1161cc-ee35-4049-8e2d-f7d63464482a",
   "metadata": {},
   "source": [
    "Here, we see a similar result to that of the sklearn mlp model where low number of neurons per layers resulted in accuracy values of 0.30-0.50. We now will test whether the accuracy does indeed increase with larger number of neurons per layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "309c0d94-cc59-44c4-9dfd-5027e5b0cce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 7.2034 - accuracy: 0.5652\n",
      "Epoch 2/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.7875 - accuracy: 0.7137\n",
      "Epoch 3/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.2997 - accuracy: 0.7518\n",
      "Epoch 4/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 1.0288 - accuracy: 0.7743\n",
      "Epoch 5/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9364 - accuracy: 0.7824\n",
      "Epoch 6/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8257 - accuracy: 0.7941\n",
      "Epoch 7/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7436 - accuracy: 0.8120\n",
      "Epoch 8/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6409 - accuracy: 0.8277\n",
      "Epoch 9/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6014 - accuracy: 0.8355\n",
      "Epoch 10/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.5652 - accuracy: 0.8397\n",
      "97/97 [==============================] - 0s 1ms/step\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 1.0123 - accuracy: 0.8217\n",
      "Accuracy of MLP trained over 10 epochs is: 0.8217\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(50,)),\n",
    "    keras.layers.Dense(50, activation=\"relu\"),\n",
    "    keras.layers.Dense(50, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "train_and_evaluate(model, X_train_50, X_test_50, y_train, y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1c06c6ca-0411-4bfd-865d-780e1658c7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 2.4847 - accuracy: 0.6334\n",
      "Epoch 2/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.8111 - accuracy: 0.7665\n",
      "Epoch 3/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.8112\n",
      "Epoch 4/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5665 - accuracy: 0.8277\n",
      "Epoch 5/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.4975 - accuracy: 0.8473\n",
      "Epoch 6/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.4626 - accuracy: 0.8575\n",
      "Epoch 7/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.4149 - accuracy: 0.8702\n",
      "Epoch 8/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.3811 - accuracy: 0.8779\n",
      "Epoch 9/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.3767 - accuracy: 0.8831\n",
      "Epoch 10/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.3796 - accuracy: 0.8829\n",
      "97/97 [==============================] - 0s 2ms/step\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.9880 - accuracy: 0.8343\n",
      "Accuracy of MLP trained over 10 epochs is: 0.8343\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(50,)),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(80, activation=\"relu\"),\n",
    "    keras.layers.Dense(70, activation=\"relu\"),\n",
    "    keras.layers.Dense(50, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "train_and_evaluate(model, X_train_50, X_test_50, y_train, y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b43ff8a5-1497-4cec-abf5-257cf3e3d37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 2.0963 - accuracy: 0.6489\n",
      "Epoch 2/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7416 - accuracy: 0.7829\n",
      "Epoch 3/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6012 - accuracy: 0.8221\n",
      "Epoch 4/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.4913 - accuracy: 0.8481\n",
      "Epoch 5/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.4413 - accuracy: 0.8620\n",
      "Epoch 6/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.3992 - accuracy: 0.8751\n",
      "Epoch 7/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.3682 - accuracy: 0.8854\n",
      "Epoch 8/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.3545 - accuracy: 0.8858\n",
      "Epoch 9/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.3036 - accuracy: 0.9027\n",
      "Epoch 10/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.3004 - accuracy: 0.9024\n",
      "97/97 [==============================] - 0s 2ms/step\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.8532 - accuracy: 0.8495\n",
      "Accuracy of MLP trained over 10 epochs is: 0.8495\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(50,)),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "train_and_evaluate(model, X_train_50, X_test_50, y_train, y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8494ed33-37b4-4820-8754-fe310c2ea184",
   "metadata": {},
   "source": [
    "While increasing only the neurons sizes as well as the number of layers, we do notice that there is a general improvement in the accuracy of the model suggesting that the available data is indeed not linear as multiple hidden layers are required to more accurately classify the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d069dc-6e31-420e-aef4-f5cd26102880",
   "metadata": {},
   "source": [
    "# Experimenting with activation functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd07805-bd06-4d96-a300-1ed3d36e1806",
   "metadata": {},
   "source": [
    "We now shall try experimenting with different combinations of layer numbers, neuron numbers as well as activation functions while keeping the other parameters the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "276ae200-c02d-41f8-b6cd-11749315bbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.3529 - accuracy: 0.5209\n",
      "Epoch 2/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 1.0537 - accuracy: 0.6178\n",
      "Epoch 3/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9624 - accuracy: 0.6579\n",
      "Epoch 4/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.9184 - accuracy: 0.6636\n",
      "Epoch 5/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8836 - accuracy: 0.6799\n",
      "Epoch 6/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8484 - accuracy: 0.6991\n",
      "Epoch 7/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8465 - accuracy: 0.7011\n",
      "Epoch 8/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8145 - accuracy: 0.7074\n",
      "Epoch 9/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7931 - accuracy: 0.7147\n",
      "Epoch 10/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7625 - accuracy: 0.7267\n",
      "97/97 [==============================] - 0s 2ms/step\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.8745 - accuracy: 0.7188\n",
      "Accuracy of MLP trained over 10 epochs is: 0.7188\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(50,)),\n",
    "    keras.layers.Dense(50, activation=\"tanh\"),\n",
    "    keras.layers.Dense(50, activation=\"tanh\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "train_and_evaluate(model, X_train_50, X_test_50, y_train, y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7bc6237d-0e17-4f52-9b62-ffb097c53895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "303/303 [==============================] - 2s 2ms/step - loss: 1.1361 - accuracy: 0.6025\n",
      "Epoch 2/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.9065 - accuracy: 0.6792\n",
      "Epoch 3/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8679 - accuracy: 0.6862\n",
      "Epoch 4/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.8787 - accuracy: 0.6752\n",
      "Epoch 5/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.8400 - accuracy: 0.6965\n",
      "Epoch 6/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.8009 - accuracy: 0.7150\n",
      "Epoch 7/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7719 - accuracy: 0.7270\n",
      "Epoch 8/10\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7584 - accuracy: 0.7258\n",
      "Epoch 9/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7660 - accuracy: 0.7275\n",
      "Epoch 10/10\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7263 - accuracy: 0.7422\n",
      "97/97 [==============================] - 0s 2ms/step\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.9190 - accuracy: 0.7061\n",
      "Accuracy of MLP trained over 10 epochs is: 0.7061\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(50,)),\n",
    "    keras.layers.Dense(100, activation=\"tanh\"),\n",
    "    keras.layers.Dense(100, activation=\"tanh\"),\n",
    "    keras.layers.Dense(50, activation=\"tanh\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "train_and_evaluate(model, X_train_50, X_test_50, y_train, y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e4b094f6-bf65-4fba-b29d-25158d3a47b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 1.1773 - accuracy: 0.5854\n",
      "Epoch 2/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.9753 - accuracy: 0.6479\n",
      "Epoch 3/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8946 - accuracy: 0.6733\n",
      "Epoch 4/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8435 - accuracy: 0.6935\n",
      "Epoch 5/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8716 - accuracy: 0.6804\n",
      "Epoch 6/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8678 - accuracy: 0.6749\n",
      "Epoch 7/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8330 - accuracy: 0.6948\n",
      "Epoch 8/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.8389 - accuracy: 0.6925\n",
      "Epoch 9/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7897 - accuracy: 0.7093\n",
      "Epoch 10/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7642 - accuracy: 0.7235\n",
      "Epoch 11/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7677 - accuracy: 0.7188\n",
      "Epoch 12/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7606 - accuracy: 0.7285\n",
      "Epoch 13/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7789 - accuracy: 0.7193\n",
      "Epoch 14/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7842 - accuracy: 0.7149\n",
      "Epoch 15/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7405 - accuracy: 0.7364\n",
      "Epoch 16/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7554 - accuracy: 0.7303\n",
      "Epoch 17/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7352 - accuracy: 0.7358\n",
      "Epoch 18/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6973 - accuracy: 0.7534\n",
      "Epoch 19/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7131 - accuracy: 0.7412\n",
      "Epoch 20/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7335 - accuracy: 0.7338\n",
      "Epoch 21/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6981 - accuracy: 0.7471\n",
      "Epoch 22/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7030 - accuracy: 0.7415\n",
      "Epoch 23/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6873 - accuracy: 0.7520\n",
      "Epoch 24/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6786 - accuracy: 0.7581\n",
      "Epoch 25/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.7577\n",
      "Epoch 26/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6856 - accuracy: 0.7529\n",
      "Epoch 27/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.7540\n",
      "Epoch 28/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6483 - accuracy: 0.7651\n",
      "Epoch 29/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.7662\n",
      "Epoch 30/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6559 - accuracy: 0.7592\n",
      "Epoch 31/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7216 - accuracy: 0.7360\n",
      "Epoch 32/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6390 - accuracy: 0.7679\n",
      "Epoch 33/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6791 - accuracy: 0.7599\n",
      "Epoch 34/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6515 - accuracy: 0.7666\n",
      "Epoch 35/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.7671\n",
      "Epoch 36/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6807 - accuracy: 0.7549\n",
      "Epoch 37/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6213 - accuracy: 0.7756\n",
      "Epoch 38/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6589 - accuracy: 0.7607\n",
      "Epoch 39/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.7747\n",
      "Epoch 40/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6943 - accuracy: 0.7411\n",
      "Epoch 41/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.7710\n",
      "Epoch 42/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.7728\n",
      "Epoch 43/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6725 - accuracy: 0.7560\n",
      "Epoch 44/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.7670\n",
      "Epoch 45/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6354 - accuracy: 0.7705\n",
      "Epoch 46/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6443 - accuracy: 0.7674\n",
      "Epoch 47/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6606 - accuracy: 0.7653\n",
      "Epoch 48/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6711 - accuracy: 0.7459\n",
      "Epoch 49/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6654 - accuracy: 0.7638\n",
      "Epoch 50/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6484 - accuracy: 0.7606\n",
      "97/97 [==============================] - 0s 2ms/step\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.7569 - accuracy: 0.7579\n",
      "Accuracy of MLP trained over 50 epochs is: 0.7579\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(50,)),\n",
    "    keras.layers.Dense(100, activation=\"tanh\"),\n",
    "    keras.layers.Dense(100, activation=\"tanh\"),\n",
    "    keras.layers.Dense(50, activation=\"tanh\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "train_and_evaluate(model, X_train_50, X_test_50, y_train, y_test, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72331afe-f278-47fc-9870-59b23dce810e",
   "metadata": {},
   "source": [
    "The tanh activation function finds difficulty in obtaining the same level of final accuracy and loss as seen by the relu activation function. When we noticed this with a smaller epoch size, we tried with a larger epoch size. In order to confirm whether this is the result of a vanishing gradient we try the same with the relu while keeping the other parameters the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dabb35ab-5baa-40f1-89e4-209cd26f5802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 2.8322 - accuracy: 0.6231\n",
      "Epoch 2/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.9291 - accuracy: 0.7506\n",
      "Epoch 3/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6997 - accuracy: 0.8061\n",
      "Epoch 4/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5942 - accuracy: 0.8280\n",
      "Epoch 5/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5305 - accuracy: 0.8409\n",
      "Epoch 6/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.5091 - accuracy: 0.8438\n",
      "Epoch 7/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.4547 - accuracy: 0.8607\n",
      "Epoch 8/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.4135 - accuracy: 0.8740\n",
      "Epoch 9/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.3740 - accuracy: 0.8846\n",
      "Epoch 10/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.3757 - accuracy: 0.8858\n",
      "Epoch 11/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.3490 - accuracy: 0.8933\n",
      "Epoch 12/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.3356 - accuracy: 0.8979\n",
      "Epoch 13/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.3244 - accuracy: 0.8989\n",
      "Epoch 14/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.3051 - accuracy: 0.9027\n",
      "Epoch 15/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.3307 - accuracy: 0.8945\n",
      "Epoch 16/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.3042 - accuracy: 0.9085\n",
      "Epoch 17/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.2843 - accuracy: 0.9124\n",
      "Epoch 18/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.2622 - accuracy: 0.9154\n",
      "Epoch 19/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.2256 - accuracy: 0.9273\n",
      "Epoch 20/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.2706 - accuracy: 0.9156\n",
      "Epoch 21/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.2891 - accuracy: 0.9086\n",
      "Epoch 22/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.2269 - accuracy: 0.9275\n",
      "Epoch 23/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.2283 - accuracy: 0.9293\n",
      "Epoch 24/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.2278 - accuracy: 0.9273\n",
      "Epoch 25/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.1994 - accuracy: 0.9359\n",
      "Epoch 26/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.2180 - accuracy: 0.9291\n",
      "Epoch 27/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.2160 - accuracy: 0.9317\n",
      "Epoch 28/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.2279 - accuracy: 0.9292\n",
      "Epoch 29/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.1992 - accuracy: 0.9363\n",
      "Epoch 30/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.1993 - accuracy: 0.9380\n",
      "Epoch 31/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.2303 - accuracy: 0.9238\n",
      "Epoch 32/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.1949 - accuracy: 0.9407\n",
      "Epoch 33/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.1896 - accuracy: 0.9392\n",
      "Epoch 34/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.1890 - accuracy: 0.9358\n",
      "Epoch 35/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.1787 - accuracy: 0.9421\n",
      "Epoch 36/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.1845 - accuracy: 0.9421\n",
      "Epoch 37/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.1908 - accuracy: 0.9387\n",
      "Epoch 38/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.1678 - accuracy: 0.9461\n",
      "Epoch 39/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.1597 - accuracy: 0.9497\n",
      "Epoch 40/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.1945 - accuracy: 0.9378\n",
      "Epoch 41/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.1727 - accuracy: 0.9416\n",
      "Epoch 42/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.1228 - accuracy: 0.9574\n",
      "Epoch 43/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.1534 - accuracy: 0.9513\n",
      "Epoch 44/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.1379 - accuracy: 0.9546\n",
      "Epoch 45/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.1500 - accuracy: 0.9504\n",
      "Epoch 46/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.1465 - accuracy: 0.9517\n",
      "Epoch 47/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.1388 - accuracy: 0.9513\n",
      "Epoch 48/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.1319 - accuracy: 0.9590\n",
      "Epoch 49/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.1648 - accuracy: 0.9439\n",
      "Epoch 50/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.1482 - accuracy: 0.9524\n",
      "97/97 [==============================] - 0s 2ms/step\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 1.2547 - accuracy: 0.8832\n",
      "Accuracy of MLP trained over 50 epochs is: 0.8832\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(50,)),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(50, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "train_and_evaluate(model, X_train_50, X_test_50, y_train, y_test, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e9c03-a6fc-44cd-bd72-bc46142e76f9",
   "metadata": {},
   "source": [
    "The results of this shows that no vanishing gradient problem is present as there is a similar drop in improvement while using only the relu activation function. We also try this with the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ff3cea70-8e25-42e8-b929-67f556b9f564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 1.5939 - accuracy: 0.4350\n",
      "Epoch 2/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 1.0462 - accuracy: 0.6585\n",
      "Epoch 3/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.9283 - accuracy: 0.6829\n",
      "Epoch 4/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8694 - accuracy: 0.7034\n",
      "Epoch 5/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.8300 - accuracy: 0.7106\n",
      "Epoch 6/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.7902 - accuracy: 0.7203\n",
      "Epoch 7/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.8019 - accuracy: 0.7187\n",
      "Epoch 8/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7709 - accuracy: 0.7331\n",
      "Epoch 9/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7601 - accuracy: 0.7362\n",
      "Epoch 10/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7231 - accuracy: 0.7501\n",
      "Epoch 11/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7200 - accuracy: 0.7470\n",
      "Epoch 12/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7244 - accuracy: 0.7353\n",
      "Epoch 13/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7156 - accuracy: 0.7452\n",
      "Epoch 14/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7130 - accuracy: 0.7462\n",
      "Epoch 15/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7043 - accuracy: 0.7508\n",
      "Epoch 16/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6754 - accuracy: 0.7569\n",
      "Epoch 17/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6599 - accuracy: 0.7640\n",
      "Epoch 18/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6792 - accuracy: 0.7573\n",
      "Epoch 19/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6831 - accuracy: 0.7545\n",
      "Epoch 20/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.7639\n",
      "Epoch 21/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6476 - accuracy: 0.7680\n",
      "Epoch 22/50\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 0.6795 - accuracy: 0.7570\n",
      "Epoch 23/50\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 0.6485 - accuracy: 0.7725\n",
      "Epoch 24/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.7706\n",
      "Epoch 25/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6105 - accuracy: 0.7839\n",
      "Epoch 26/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6179 - accuracy: 0.7850\n",
      "Epoch 27/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.5995 - accuracy: 0.7893\n",
      "Epoch 28/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6059 - accuracy: 0.7842\n",
      "Epoch 29/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6166 - accuracy: 0.7775\n",
      "Epoch 30/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.5878 - accuracy: 0.7893\n",
      "Epoch 31/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.6096 - accuracy: 0.7856\n",
      "Epoch 32/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5962 - accuracy: 0.7899\n",
      "Epoch 33/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.5669 - accuracy: 0.7966\n",
      "Epoch 34/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.5595 - accuracy: 0.8037\n",
      "Epoch 35/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.5587 - accuracy: 0.8059\n",
      "Epoch 36/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5620 - accuracy: 0.8008\n",
      "Epoch 37/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5576 - accuracy: 0.8027\n",
      "Epoch 38/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6097 - accuracy: 0.7901\n",
      "Epoch 39/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5987 - accuracy: 0.7884\n",
      "Epoch 40/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5611 - accuracy: 0.8047\n",
      "Epoch 41/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5728 - accuracy: 0.7972\n",
      "Epoch 42/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5757 - accuracy: 0.7998\n",
      "Epoch 43/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5896 - accuracy: 0.7898\n",
      "Epoch 44/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6102 - accuracy: 0.7845\n",
      "Epoch 45/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5616 - accuracy: 0.8050\n",
      "Epoch 46/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.5642 - accuracy: 0.8057\n",
      "Epoch 47/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5842 - accuracy: 0.7923\n",
      "Epoch 48/50\n",
      "303/303 [==============================] - 1s 2ms/step - loss: 0.5612 - accuracy: 0.7970\n",
      "Epoch 49/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5423 - accuracy: 0.8127\n",
      "Epoch 50/50\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5616 - accuracy: 0.8018\n",
      "97/97 [==============================] - 0s 2ms/step\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.7854\n",
      "Accuracy of MLP trained over 50 epochs is: 0.7854\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(50,)),\n",
    "    keras.layers.Dense(100, activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(100, activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(50, activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "train_and_evaluate(model, X_train_50, X_test_50, y_train, y_test, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8466581-fb6e-44c4-8bef-cb4ef0a0c8cd",
   "metadata": {},
   "source": [
    "Although the sigmoid function seems to be performing slightly better than the tanh function, it still seems to suffer from a similar problem where the results seem to not improve as much during the later epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2a640e-0888-41c3-8b32-d95c3ba49c8c",
   "metadata": {},
   "source": [
    "Trying a variety of activation functions on the same network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2b8cbb8c-102c-45bc-b7e0-4cc01631ebc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 1.0926 - accuracy: 0.6130\n",
      "Epoch 2/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.8911 - accuracy: 0.6776\n",
      "Epoch 3/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.8757 - accuracy: 0.6793\n",
      "Epoch 4/30\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 0.8545 - accuracy: 0.6916\n",
      "Epoch 5/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7938 - accuracy: 0.7074\n",
      "Epoch 6/30\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 0.7870 - accuracy: 0.7149\n",
      "Epoch 7/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.8003 - accuracy: 0.7103\n",
      "Epoch 8/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7913 - accuracy: 0.7133\n",
      "Epoch 9/30\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 0.7768 - accuracy: 0.7152\n",
      "Epoch 10/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7347 - accuracy: 0.7404\n",
      "Epoch 11/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7151 - accuracy: 0.7450\n",
      "Epoch 12/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6836 - accuracy: 0.7464\n",
      "Epoch 13/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7112 - accuracy: 0.7395\n",
      "Epoch 14/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.7714\n",
      "Epoch 15/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6310 - accuracy: 0.7719\n",
      "Epoch 16/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.7650\n",
      "Epoch 17/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.7676\n",
      "Epoch 18/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6864 - accuracy: 0.7542\n",
      "Epoch 19/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6779 - accuracy: 0.7529\n",
      "Epoch 20/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6586 - accuracy: 0.7634\n",
      "Epoch 21/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6800 - accuracy: 0.7458\n",
      "Epoch 22/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.7789\n",
      "Epoch 23/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6123 - accuracy: 0.7788\n",
      "Epoch 24/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6099 - accuracy: 0.7852\n",
      "Epoch 25/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.7781\n",
      "Epoch 26/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5709 - accuracy: 0.7988\n",
      "Epoch 27/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5832 - accuracy: 0.7895\n",
      "Epoch 28/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5756 - accuracy: 0.7909\n",
      "Epoch 29/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5792 - accuracy: 0.7893\n",
      "Epoch 30/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.7713\n",
      "97/97 [==============================] - 0s 2ms/step\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.7162 - accuracy: 0.7832\n",
      "Accuracy of MLP trained over 30 epochs is: 0.7832\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(50,)),\n",
    "    keras.layers.Dense(100, activation=\"tanh\"),\n",
    "    keras.layers.Dense(200, activation=\"relu\"),\n",
    "    keras.layers.Dense(500, activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "train_and_evaluate(model, X_train_50, X_test_50, y_train, y_test, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "64ae273b-821c-4783-8ac7-f5c2ea7f3d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "303/303 [==============================] - 2s 3ms/step - loss: 1.1764 - accuracy: 0.5783\n",
      "Epoch 2/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.9085 - accuracy: 0.6670\n",
      "Epoch 3/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.8521 - accuracy: 0.6874\n",
      "Epoch 4/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7959 - accuracy: 0.7150\n",
      "Epoch 5/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7552 - accuracy: 0.7285\n",
      "Epoch 6/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7245 - accuracy: 0.7422\n",
      "Epoch 7/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.7172 - accuracy: 0.7428\n",
      "Epoch 8/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6999 - accuracy: 0.7513\n",
      "Epoch 9/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6872 - accuracy: 0.7519\n",
      "Epoch 10/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6888 - accuracy: 0.7585\n",
      "Epoch 11/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6861 - accuracy: 0.7543\n",
      "Epoch 12/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.7646\n",
      "Epoch 13/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6209 - accuracy: 0.7773\n",
      "Epoch 14/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6174 - accuracy: 0.7805\n",
      "Epoch 15/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6018 - accuracy: 0.7837\n",
      "Epoch 16/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6124 - accuracy: 0.7816\n",
      "Epoch 17/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.7727\n",
      "Epoch 18/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5672 - accuracy: 0.7929\n",
      "Epoch 19/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5715 - accuracy: 0.8001\n",
      "Epoch 20/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5470 - accuracy: 0.8041\n",
      "Epoch 21/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5850 - accuracy: 0.7916\n",
      "Epoch 22/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5520 - accuracy: 0.8034\n",
      "Epoch 23/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5450 - accuracy: 0.8051\n",
      "Epoch 24/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5107 - accuracy: 0.8178\n",
      "Epoch 25/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5262 - accuracy: 0.8132\n",
      "Epoch 26/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5498 - accuracy: 0.8070\n",
      "Epoch 27/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5328 - accuracy: 0.8103\n",
      "Epoch 28/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5282 - accuracy: 0.8136\n",
      "Epoch 29/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.5208 - accuracy: 0.8160\n",
      "Epoch 30/30\n",
      "303/303 [==============================] - 1s 3ms/step - loss: 0.4847 - accuracy: 0.8301\n",
      "97/97 [==============================] - 0s 2ms/step\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.5914 - accuracy: 0.8165\n",
      "Accuracy of MLP trained over 30 epochs is: 0.8165\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(50,)),\n",
    "    keras.layers.Dense(100, activation=\"sigmoid\"),\n",
    "    keras.layers.Dense(50, activation=\"relu\"),\n",
    "    keras.layers.Dense(50, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"tanh\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "train_and_evaluate(model, X_train_50, X_test_50, y_train, y_test, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
